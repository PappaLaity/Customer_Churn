# Testing Guide: Drift-Based Retraining\n\nThis guide explains how to run and understand the tests for the drift-based retraining implementation.\n\n## Quick Start\n\n### Run All Core Tests\n```bash\n# Unit tests for retraining logic\npython3 -m pytest tests/test_retrain.py -v\n\n# Integration tests (no Airflow required)\npython3 test_drift_retrain_logic.py\n```\n\n## Test Files\n\n### 1. `tests/test_retrain.py` - Unit Tests (18 tests)\n\nComprehensive unit tests for the retraining module.\n\n**Run individually:**\n```bash\n# Run all tests\npython3 -m pytest tests/test_retrain.py -v\n\n# Run specific test class\npython3 -m pytest tests/test_retrain.py::TestTrainCombined -v\n\n# Run specific test\npython3 -m pytest tests/test_retrain.py::TestTrainCombined::test_train_combined_empty_production_returns_minus_one -v\n```\n\n**Coverage:**\n- Label validation\n- Data alignment and concatenation\n- Train-test splitting with SMOTE\n- Combined training with production data\n- Feature-only training\n- Error handling for missing/empty files\n\n### 2. `test_drift_retrain_logic.py` - Integration Tests (4 tests)\n\nStandalone integration tests that verify the complete drift-based workflow without requiring Airflow.\n\n**Run:**\n```bash\npython3 test_drift_retrain_logic.py\n```\n\n**Tests:**\n1. **No Drift Scenario**: Verifies retraining is skipped when production data is empty\n2. **Drift Scenario**: Verifies retraining proceeds when production data exists\n3. **Missing Production File**: Verifies handling of missing production files\n4. **Branch Logic**: Verifies DAG branching logic\n\n### 3. `tests/test_drift_retrain_dag.py` - DAG Tests (8 tests)\n\nTests for the Airflow DAG structure. Requires Airflow to be installed.\n\n**Run (requires Airflow):**\n```bash\npython3 -m pytest tests/test_drift_retrain_dag.py -v\n```\n\n**Features:**\n- DAG import and structure validation\n- Task dependency verification\n- Branching logic tests\n- Drift detection function tests\n\n## Key Test Scenarios\n\n### Scenario 1: No Drift (Retraining Skipped)\n\n**Condition:** Production data is empty or missing\n**Expected Behavior:** Return `-1` and skip retraining\n\n```bash\npython3 -m pytest tests/test_retrain.py::TestTrainCombined::test_train_combined_empty_production_returns_minus_one -v\n```\n\n### Scenario 2: Drift Detected (Retraining Proceeds)\n\n**Condition:** Production data exists with records\n**Expected Behavior:** Proceed with training and return model version\n\n```bash\npython3 -m pytest tests/test_retrain.py::TestDriftSkipScenario::test_retrain_when_production_data_exists -v\n```\n\n### Scenario 3: DAG Branching\n\n**Condition:** Drift detection completed\n**Expected Behavior:** Route to `retrain_combined` or `skip_retraining` based on drift flag\n\n```bash\npython3 test_drift_retrain_logic.py  # Tests the branching logic\n```\n\n## Interpretation of Test Results\n\n### Passing Tests (✅)\n\n```\n18 passed in 1.75s\n```\n\nAll tests executed successfully. The drift-based retraining logic is working as expected.\n\n### Failed Tests (❌)\n\nIf a test fails, check:\n\n1. **Test output** - Look for assertion errors\n2. **Error message** - Understand what was expected vs. actual\n3. **Relevant code** - Review the modified files\n\n**Example failure:**\n```\nFAILED tests/test_retrain.py::TestTrainCombined::test_train_combined_empty_production_returns_minus_one\nAssertionError: assert 0 == -1\n```\n\nThis would indicate that the function returned `0` instead of `-1`, meaning the no-drift scenario handling isn't working correctly.\n\n## Test Dependencies\n\n### Required Packages\n- `pytest` - Test framework\n- `pandas` - Data handling\n- `numpy` - Numerical operations\n- `scikit-learn` - ML utilities\n- `imbalanced-learn` - SMOTE implementation\n\n### Optional\n- `airflow` - For DAG tests (only needed for `test_drift_retrain_dag.py`)\n\n## Continuous Integration\n\n### GitHub Actions Example\n\nAdd to `.github/workflows/test.yml`:\n\n```yaml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n        with:\n          python-version: '3.11'\n      - name: Install dependencies\n        run: pip install -r requirements.txt pytest\n      - name: Run unit tests\n        run: python3 -m pytest tests/test_retrain.py -v\n      - name: Run integration tests\n        run: python3 test_drift_retrain_logic.py\n```\n\n## Debugging Failed Tests\n\n### Enable Verbose Output\n\n```bash\n# Show all print statements and logs\npython3 -m pytest tests/test_retrain.py -v -s\n\n# Show local variables in tracebacks\npython3 -m pytest tests/test_retrain.py -v -l\n\n# Full traceback\npython3 -m pytest tests/test_retrain.py -v --tb=long\n```\n\n### Run Single Test with Debugging\n\n```bash\n# Run with pdb (Python debugger)\npython3 -m pytest tests/test_retrain.py::TestTrainCombined::test_train_combined_empty_production_returns_minus_one -v --pdb\n```\n\n### Check Test Coverage\n\n```bash\n# Install pytest-cov\npip install pytest-cov\n\n# Run with coverage\npython3 -m pytest tests/test_retrain.py --cov=src.training.retrain --cov-report=html\n\n# View report\nopen htmlcov/index.html\n```\n\n## Common Issues and Solutions\n\n### Issue: \"ModuleNotFoundError: No module named 'src'\"\n\n**Solution:** Make sure you're running tests from the project root:\n```bash\ncd /Users/mahamatabakarassouna/projects/Customer_Churn\npython3 -m pytest tests/test_retrain.py\n```\n\n### Issue: \"ModuleNotFoundError: No module named 'airflow'\"\n\n**Solution:** Airflow is only needed for DAG tests. Skip them if Airflow isn't installed:\n```bash\npython3 -m pytest tests/test_retrain.py  # Runs only retrain tests\npython3 test_drift_retrain_logic.py       # Runs integration tests\n```\n\n### Issue: \"EmptyDataError: No columns to parse from file\"\n\n**Solution:** This is expected in some tests. The test is verifying proper error handling:\n```bash\n# This test should pass - it's testing error handling\npython3 -m pytest tests/test_retrain.py::TestTrainCombined::test_train_combined_empty_features_raises_error -v\n```\n\n## Test Maintenance\n\n### When to Update Tests\n\n1. **When modifying retrain.py**: Update corresponding tests in `test_retrain.py`\n2. **When modifying drift_retrain_dag.py**: Update corresponding tests in `test_drift_retrain_dag.py`\n3. **When adding new functionality**: Add new test methods to relevant test classes\n\n### Best Practices\n\n- Write tests before writing code (TDD)\n- Test both success and failure scenarios\n- Use descriptive test names\n- Mock external dependencies (MLflow, file I/O)\n- Clean up temporary files in tests\n- Document test purpose in docstrings\n\n## Performance Testing\n\nFor testing with large datasets:\n\n```bash\n# Create a performance test\ncat > test_performance.py << 'EOF'\nimport time\nimport pandas as pd\nimport numpy as np\nfrom src.training.retrain import _split_scale_smote\n\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'Age': np.random.randint(18, 80, 100000),\n    'Tenure': np.random.randint(1, 72, 100000),\n    'Churn': np.random.randint(0, 2, 100000),\n})\n\nstart = time.time()\nX_train, X_test, y_train, y_test = _split_scale_smote(df)\nend = time.time()\n\nprint(f\"Processing time: {end - start:.2f}s\")\nprint(f\"Training set size: {len(X_train)}\")\nEOF\n\npython3 test_performance.py\n```\n\n## Additional Resources\n\n- [Pytest Documentation](https://docs.pytest.org/)\n- [Python unittest Documentation](https://docs.python.org/3/library/unittest.html)\n- [TEST_SUMMARY.md](TEST_SUMMARY.md) - Detailed test summary\n